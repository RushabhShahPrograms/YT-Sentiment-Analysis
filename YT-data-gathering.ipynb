{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
      "     ---------------------------------------- 0.0/622.8 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/622.8 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/622.8 kB ? eta -:--:--\n",
      "     - ----------------------------------- 30.7/622.8 kB 330.3 kB/s eta 0:00:02\n",
      "     --- --------------------------------- 61.4/622.8 kB 409.6 kB/s eta 0:00:02\n",
      "     ------- ---------------------------- 122.9/622.8 kB 602.4 kB/s eta 0:00:01\n",
      "     -------- --------------------------- 143.4/622.8 kB 610.6 kB/s eta 0:00:01\n",
      "     ---------- ------------------------- 174.1/622.8 kB 583.1 kB/s eta 0:00:01\n",
      "     -------------- --------------------- 245.8/622.8 kB 718.0 kB/s eta 0:00:01\n",
      "     ---------------- ------------------- 286.7/622.8 kB 770.1 kB/s eta 0:00:01\n",
      "     ------------------ ----------------- 327.7/622.8 kB 813.9 kB/s eta 0:00:01\n",
      "     ----------------------- ------------ 409.6/622.8 kB 853.3 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 430.1/622.8 kB 815.2 kB/s eta 0:00:01\n",
      "     ---------------------------- ------- 491.5/622.8 kB 856.4 kB/s eta 0:00:01\n",
      "     -------------------------------- --- 563.2/622.8 kB 932.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ 622.8/622.8 kB 933.7 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py): started\n",
      "  Building wheel for autocorrect (setup.py): finished with status 'done'\n",
      "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622390 sha256=1c3ad6638a7515f607d79445b6e940b4baec83a358db3725b6d2f7411df3b8d0\n",
      "  Stored in directory: c:\\users\\rushabh\\appdata\\local\\pip\\cache\\wheels\\5e\\90\\99\\807a5ad861ce5d22c3c299a11df8cba9f31524f23ae6e645cb\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.11.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
      "   ---------------------------------------- 0.0/433.8 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/433.8 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/433.8 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 41.0/433.8 kB 393.8 kB/s eta 0:00:01\n",
      "   ------ -------------------------------- 71.7/433.8 kB 438.9 kB/s eta 0:00:01\n",
      "   ---------- --------------------------- 122.9/433.8 kB 654.9 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 174.1/433.8 kB 700.2 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 276.5/433.8 kB 1.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 368.6/433.8 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 433.8/433.8 kB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.65.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\rushabh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "!pip install pandas\n",
    "!pip install autocorrect\n",
    "!pip install emoji\n",
    "!pip install nltk\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RUSHABH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RUSHABH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from autocorrect import Speller\n",
    "import emoji\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key = \"\"\n",
    "api_key = \"\"\n",
    "# Build the YouTube client\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get IDs from the youtube URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose an option:\n",
      "1. Enter YouTube video URLs\n",
      "2. Enter YouTube playlist URLs\n",
      "Extracted video IDs: ['Qy0Q_AYs63Y']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_youtube_ids(input_string):\n",
    "    # Regular expression pattern to match YouTube video or playlist IDs\n",
    "    pattern = r'(?:https?://)?(?:www\\.)?(?:youtube\\.com/(?:watch\\?v=|playlist\\?list=)|youtu.be/)([a-zA-Z0-9_-]+)'\n",
    "    \n",
    "    # Find all matches in the input string\n",
    "    matches = re.findall(pattern, input_string)\n",
    "    \n",
    "    # Return the list of matches\n",
    "    return matches\n",
    "\n",
    "# Function to get user input for YouTube videos\n",
    "def get_youtube_videos():\n",
    "    input_string = input(\"Enter YouTube video URLs separated by commas: \")\n",
    "    video_ids = extract_youtube_ids(input_string)\n",
    "    return video_ids\n",
    "\n",
    "# Function to get user input for YouTube playlists\n",
    "def get_youtube_playlists():\n",
    "    input_string = input(\"Enter YouTube playlist URLs separated by commas: \")\n",
    "    playlist_ids = extract_youtube_ids(input_string)\n",
    "    return playlist_ids\n",
    "\n",
    "# Main function\n",
    "\n",
    "print(\"Choose an option:\")\n",
    "print(\"1. Enter YouTube video URLs\")\n",
    "print(\"2. Enter YouTube playlist URLs\")\n",
    "choice = input(\"Enter your choice (1 or 2): \")\n",
    "\n",
    "if choice == '1':\n",
    "    video_ids = get_youtube_videos()\n",
    "    print(\"Extracted video IDs:\", video_ids)\n",
    "elif choice == '2':\n",
    "    playlist_ids = get_youtube_playlists()\n",
    "    print(\"Extracted playlist IDs:\", playlist_ids)\n",
    "else:\n",
    "    print(\"Invalid choice. Please enter either 1 or 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get video_ids from playlist_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_video_ids_from_playlists(youtube, playlist_ids):\n",
    "    all_videos = []  # Initialize a single list to hold all video IDs\n",
    "\n",
    "    for playlist_id in playlist_ids:\n",
    "        next_page_token = None\n",
    "\n",
    "        # Fetch videos from the current playlist\n",
    "        while True:\n",
    "            playlist_request = youtube.playlistItems().list(\n",
    "                part='contentDetails',\n",
    "                playlistId=playlist_id,\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token)\n",
    "            playlist_response = playlist_request.execute()\n",
    "\n",
    "            all_videos += [item['contentDetails']['videoId'] for item in playlist_response['items']]\n",
    "\n",
    "            next_page_token = playlist_response.get('nextPageToken')\n",
    "\n",
    "            if next_page_token is None:\n",
    "                break\n",
    "\n",
    "    return all_videos\n",
    "\n",
    "if playlist_ids == True:\n",
    "    video_ids = get_all_video_ids_from_playlists(youtube, playlist_ids)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Youtube videos we are going to extract all the comments from the comment section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get replies for a specific comment\n",
    "def get_replies(youtube, parent_id, video_ids):  # Added video_id as an argument\n",
    "    replies = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        reply_request = youtube.comments().list(\n",
    "            part=\"snippet\",\n",
    "            parentId=parent_id,\n",
    "            textFormat=\"plainText\",\n",
    "            maxResults=100,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        reply_response = reply_request.execute()\n",
    "\n",
    "        for item in reply_response['items']:\n",
    "            comment = item['snippet']\n",
    "            replies.append({\n",
    "                'Timestamp': comment['publishedAt'],\n",
    "                'Username': comment['authorDisplayName'],\n",
    "                'VideoID': video_ids,\n",
    "                'Comment': comment['textDisplay'],\n",
    "                'Date': comment['updatedAt'] if 'updatedAt' in comment else comment['publishedAt']\n",
    "            })\n",
    "\n",
    "        next_page_token = reply_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return replies\n",
    "\n",
    "\n",
    "# Function to get all comments (including replies) for a single video\n",
    "def get_comments_for_video(youtube, video_ids):\n",
    "    all_comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        comment_request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_ids,\n",
    "            pageToken=next_page_token,\n",
    "            textFormat=\"plainText\",\n",
    "            maxResults=100\n",
    "        )\n",
    "        comment_response = comment_request.execute()\n",
    "\n",
    "        for item in comment_response['items']:\n",
    "            top_comment = item['snippet']['topLevelComment']['snippet']\n",
    "            all_comments.append({\n",
    "                'Timestamp': top_comment['publishedAt'],\n",
    "                'Username': top_comment['authorDisplayName'],\n",
    "                'VideoID': video_ids,  # Directly using video_id from function parameter\n",
    "                'Comment': top_comment['textDisplay'],\n",
    "                'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt']\n",
    "            })\n",
    "\n",
    "            # Fetch replies if there are any\n",
    "            # if item['snippet']['totalReplyCount'] > 0:\n",
    "                # all_comments.extend(get_replies(youtube, item['snippet']['topLevelComment']['id'], video_ids))\n",
    "\n",
    "        next_page_token = comment_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold all comments from all videos\n",
    "# all_comments = []\n",
    "\n",
    "\n",
    "# for video_id in video_ids:\n",
    "    # video_comments = get_comments_for_video(youtube, video_id)\n",
    "    # all_comments.extend(video_comments)\n",
    "\n",
    "# Create DataFrame\n",
    "# comments_df = pd.DataFrame(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove emojis from text\n",
    "def clean_text(text):\n",
    "    # Use a regex pattern to match and remove emojis, punctuation, and symbols\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               u\"\\U000024C2-\\U0001F251\"  # Miscellaneous Symbols and Pictographs\n",
    "                               \"\\s\"  # whitespace\n",
    "                               \"{}\"   # additional symbols to remove, e.g., punctuation\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# List of additional symbols to remove (punctuation)\n",
    "additional_symbols_to_remove = string.punctuation.replace('!', '').replace('?', '').replace(';', '').replace(':', '').replace(',','')\n",
    "\n",
    "\n",
    "# Update the emoji removal function with additional symbols\n",
    "def remove_emojis_and_symbols(text):\n",
    "    escaped_text = text.replace('{', '{{').replace('}', '}}')  # Escape curly braces\n",
    "    return clean_text(escaped_text.format(additional_symbols_to_remove))\n",
    "\n",
    "# Function to remove stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Function to autocorrect words\n",
    "# def autocorrect_text(text):\n",
    "#     spell = Speller()\n",
    "#     words = word_tokenize(text)\n",
    "#     corrected_words = [spell(word) for word in words]\n",
    "#     return ' '.join(corrected_words)\n",
    "\n",
    "# List to hold all comments from all videos\n",
    "all_processed_comments = []\n",
    "\n",
    "for video_id in video_ids:\n",
    "    video_comments = get_comments_for_video(youtube, video_id)\n",
    "    \n",
    "    # Preprocess comments\n",
    "    processed_comments = []\n",
    "    for comment in video_comments:\n",
    "        comment_text = comment['Comment']\n",
    "        comment_text = remove_emojis_and_symbols(comment_text)\n",
    "        comment_text = remove_stopwords(comment_text)\n",
    "        # comment_text = autocorrect_text(comment_text)\n",
    "        comment['Comment'] = comment_text\n",
    "        processed_comments.append(comment)\n",
    "    \n",
    "    all_processed_comments.extend(processed_comments)\n",
    "\n",
    "# Create DataFrame\n",
    "comments_df = pd.DataFrame(all_processed_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13165"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_df)\n",
    "\n",
    "# sentiment_labels = ['positive'] * len(comments_df)\n",
    "\n",
    "# Add sentiment labels as a new column in the DataFrame\n",
    "# comments_df['Sentiment'] = sentiment_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Username</th>\n",
       "      <th>VideoID</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-09-06T14:30:52Z</td>\n",
       "      <td>@PhysicsWallah</td>\n",
       "      <td>Qy0Q_AYs63Y</td>\n",
       "      <td>LiveClasses , VideoLectures , TestSeries , Lec...</td>\n",
       "      <td>2020-09-10T05:37:50Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-07T14:12:39Z</td>\n",
       "      <td>@khushi_dagur</td>\n",
       "      <td>Qy0Q_AYs63Y</td>\n",
       "      <td>Anyonefrom2024-25batch</td>\n",
       "      <td>2024-04-07T14:12:39Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-07T10:03:37Z</td>\n",
       "      <td>@vecna_jd14</td>\n",
       "      <td>Qy0Q_AYs63Y</td>\n",
       "      <td>Class10thchemistry-Class11thchemistry-</td>\n",
       "      <td>2024-04-07T10:03:37Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-07T09:05:36Z</td>\n",
       "      <td>@maseerasajal118</td>\n",
       "      <td>Qy0Q_AYs63Y</td>\n",
       "      <td>2025-2026batchattendencedo</td>\n",
       "      <td>2024-04-07T09:05:36Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-07T06:33:45Z</td>\n",
       "      <td>@sakshi.rajvanshi.75.</td>\n",
       "      <td>Qy0Q_AYs63Y</td>\n",
       "      <td>Attendancetoclass11th2024and2025batch</td>\n",
       "      <td>2024-04-07T06:34:03Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13160</th>\n",
       "      <td>2018-05-02T05:24:22Z</td>\n",
       "      <td>@arabindapramanik7434</td>\n",
       "      <td>Qy0Q_AYs63Y</td>\n",
       "      <td>Thankssir</td>\n",
       "      <td>2018-05-02T05:24:22Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13161</th>\n",
       "      <td>2018-05-02T05:24:01Z</td>\n",
       "      <td>@subhajitmondal5955</td>\n",
       "      <td>Qy0Q_AYs63Y</td>\n",
       "      <td>physicsclass11kabvideouploadkarenge</td>\n",
       "      <td>2018-05-02T05:24:01Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13162</th>\n",
       "      <td>2018-05-02T05:23:40Z</td>\n",
       "      <td>@akankshasingh3081</td>\n",
       "      <td>Qy0Q_AYs63Y</td>\n",
       "      <td>Thanksagainsir</td>\n",
       "      <td>2018-05-02T05:23:40Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13163</th>\n",
       "      <td>2018-05-02T05:22:57Z</td>\n",
       "      <td>@anujrai5894</td>\n",
       "      <td>Qy0Q_AYs63Y</td>\n",
       "      <td>Thanksyousir</td>\n",
       "      <td>2018-05-02T05:22:57Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13164</th>\n",
       "      <td>2018-05-02T05:22:41Z</td>\n",
       "      <td>@angelru3119</td>\n",
       "      <td>Qy0Q_AYs63Y</td>\n",
       "      <td>Sir1stonetolykcommentandview ...... Sirawaitin...</td>\n",
       "      <td>2018-05-02T05:23:20Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13165 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Timestamp               Username      VideoID  \\\n",
       "0      2018-09-06T14:30:52Z         @PhysicsWallah  Qy0Q_AYs63Y   \n",
       "1      2024-04-07T14:12:39Z          @khushi_dagur  Qy0Q_AYs63Y   \n",
       "2      2024-04-07T10:03:37Z            @vecna_jd14  Qy0Q_AYs63Y   \n",
       "3      2024-04-07T09:05:36Z       @maseerasajal118  Qy0Q_AYs63Y   \n",
       "4      2024-04-07T06:33:45Z  @sakshi.rajvanshi.75.  Qy0Q_AYs63Y   \n",
       "...                     ...                    ...          ...   \n",
       "13160  2018-05-02T05:24:22Z  @arabindapramanik7434  Qy0Q_AYs63Y   \n",
       "13161  2018-05-02T05:24:01Z    @subhajitmondal5955  Qy0Q_AYs63Y   \n",
       "13162  2018-05-02T05:23:40Z     @akankshasingh3081  Qy0Q_AYs63Y   \n",
       "13163  2018-05-02T05:22:57Z           @anujrai5894  Qy0Q_AYs63Y   \n",
       "13164  2018-05-02T05:22:41Z           @angelru3119  Qy0Q_AYs63Y   \n",
       "\n",
       "                                                 Comment                  Date  \n",
       "0      LiveClasses , VideoLectures , TestSeries , Lec...  2020-09-10T05:37:50Z  \n",
       "1                                 Anyonefrom2024-25batch  2024-04-07T14:12:39Z  \n",
       "2                 Class10thchemistry-Class11thchemistry-  2024-04-07T10:03:37Z  \n",
       "3                             2025-2026batchattendencedo  2024-04-07T09:05:36Z  \n",
       "4                  Attendancetoclass11th2024and2025batch  2024-04-07T06:34:03Z  \n",
       "...                                                  ...                   ...  \n",
       "13160                                          Thankssir  2018-05-02T05:24:22Z  \n",
       "13161                physicsclass11kabvideouploadkarenge  2018-05-02T05:24:01Z  \n",
       "13162                                     Thanksagainsir  2018-05-02T05:23:40Z  \n",
       "13163                                       Thanksyousir  2018-05-02T05:22:57Z  \n",
       "13164  Sir1stonetolykcommentandview ...... Sirawaitin...  2018-05-02T05:23:20Z  \n",
       "\n",
       "[13165 rows x 5 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head(80000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have a DataFrame named 'comments_df' with 'Comment' and 'Sentiment' columns\n",
    "# 'Comment' column contains the preprocessed comments and 'Sentiment' column contains the sentiment labels\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(comments_df['Comment'], comments_df['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating a CountVectorizer to convert comments into numerical feature vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Training the Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = nb_classifier.predict(X_test_vec)\n",
    "\n",
    "# Calculating accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Naive Bayes Classifier Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
