{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RUSHABH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RUSHABH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from autocorrect import Speller\n",
    "import emoji\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding YouTube API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key = \"AIzaSyCw2Jv32VeeGUx3cd_t1E85XlQ-KpZGCBM\"\n",
    "api_key = \"AIzaSyARddhV0r2iP7sdnb9yWdjDxvceZw8ViL0\"\n",
    "# api_key = \"AIzaSyDG1D34FAEtIVK_o9JcjWD_owY-v2LG8os\"\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting YouTube video ID from the URL.\n",
    "\n",
    "- Type 1 for YouTube videos\n",
    "- Type 2 for YouTube playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose an option:\n",
      "1. Enter YouTube video URLs\n",
      "2. Enter YouTube playlist URLs\n",
      "Extracted playlist IDs: ['PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_youtube_ids(input_string):\n",
    "    # Regular expression pattern to match YouTube video or playlist IDs\n",
    "    pattern = r'(?:https?://)?(?:www\\.)?(?:youtube\\.com/(?:watch\\?v=|playlist\\?list=)|youtu.be/)([a-zA-Z0-9_-]+)'\n",
    "    \n",
    "    # Find all matches in the input string\n",
    "    matches = re.findall(pattern, input_string)\n",
    "    \n",
    "    # Return the list of matches\n",
    "    return matches\n",
    "\n",
    "# Function to get user input for YouTube videos\n",
    "def get_youtube_videos():\n",
    "    input_string = input(\"Enter YouTube video URLs separated by commas: \")\n",
    "    video_ids = extract_youtube_ids(input_string)\n",
    "    return video_ids\n",
    "\n",
    "# Function to get user input for YouTube playlists\n",
    "def get_youtube_playlists():\n",
    "    input_string = input(\"Enter YouTube playlist URLs separated by commas: \")\n",
    "    playlist_ids = extract_youtube_ids(input_string)\n",
    "    return playlist_ids\n",
    "\n",
    "# Main function\n",
    "\n",
    "print(\"Choose an option:\")\n",
    "print(\"1. Enter YouTube video URLs\")\n",
    "print(\"2. Enter YouTube playlist URLs\")\n",
    "choice = input(\"Enter your choice (1 or 2): \")\n",
    "\n",
    "if choice == '1':\n",
    "    video_ids = get_youtube_videos()\n",
    "    print(\"Extracted video IDs:\", video_ids)\n",
    "elif choice == '2':\n",
    "    playlist_ids = get_youtube_playlists()\n",
    "    print(\"Extracted playlist IDs:\", playlist_ids)\n",
    "else:\n",
    "    print(\"Invalid choice. Please enter either 1 or 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Video IDs from the playlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'playlist_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_videos\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Fetch all video IDs from the specified playlists\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m video_ids \u001b[38;5;241m=\u001b[39m get_all_video_ids_from_playlists(youtube, \u001b[43mplaylist_ids\u001b[49m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Now you can pass video_ids to the next function\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# next_function(video_ids)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'playlist_ids' is not defined"
     ]
    }
   ],
   "source": [
    "def get_all_video_ids_from_playlists(youtube, playlist_ids):\n",
    "    all_videos = []  # Initialize a single list to hold all video IDs\n",
    "\n",
    "    for playlist_id in playlist_ids:\n",
    "        next_page_token = None\n",
    "\n",
    "        # Fetch videos from the current playlist\n",
    "        while True:\n",
    "            playlist_request = youtube.playlistItems().list(\n",
    "                part='contentDetails',\n",
    "                playlistId=playlist_id,\n",
    "                maxResults=50,\n",
    "                pageToken=next_page_token)\n",
    "            playlist_response = playlist_request.execute()\n",
    "\n",
    "            all_videos += [item['contentDetails']['videoId'] for item in playlist_response['items']]\n",
    "\n",
    "            next_page_token = playlist_response.get('nextPageToken')\n",
    "\n",
    "            if next_page_token is None:\n",
    "                break\n",
    "\n",
    "    return all_videos\n",
    "\n",
    "# Fetch all video IDs from the specified playlists\n",
    "video_ids = get_all_video_ids_from_playlists(youtube, playlist_ids)\n",
    "\n",
    "# Now you can pass video_ids to the next function\n",
    "# next_function(video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mvideo_ids\u001b[49m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(video_ids)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'video_ids' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(video_ids))\n",
    "print(video_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching replies from each and every videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 67\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# List to hold all comments from all videos\u001b[39;00m\n\u001b[0;32m     64\u001b[0m all_comments \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvideo_ids\u001b[49m:\n\u001b[0;32m     68\u001b[0m     video_comments \u001b[38;5;241m=\u001b[39m get_comments_for_video(youtube, video_id)\n\u001b[0;32m     69\u001b[0m     all_comments\u001b[38;5;241m.\u001b[39mextend(video_comments)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'video_ids' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to get replies for a specific comment\n",
    "def get_replies(youtube, parent_id, video_id):  # Added video_id as an argument\n",
    "    replies = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        reply_request = youtube.comments().list(\n",
    "            part=\"snippet\",\n",
    "            parentId=parent_id,\n",
    "            textFormat=\"plainText\",\n",
    "            maxResults=100,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        reply_response = reply_request.execute()\n",
    "\n",
    "        for item in reply_response['items']:\n",
    "            comment = item['snippet']\n",
    "            replies.append({\n",
    "                'Timestamp': comment['publishedAt'],\n",
    "                'Username': comment['authorDisplayName'],\n",
    "                'VideoID': video_id,\n",
    "                'Comment': comment['textDisplay'],\n",
    "                'Date': comment['updatedAt'] if 'updatedAt' in comment else comment['publishedAt']\n",
    "            })\n",
    "\n",
    "        next_page_token = reply_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return replies\n",
    "\n",
    "# Function to get all top-level comments for a single video\n",
    "def get_comments_for_video(youtube, video_id):\n",
    "    all_comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        comment_request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            pageToken=next_page_token,\n",
    "            textFormat=\"plainText\",\n",
    "            maxResults=100\n",
    "        )\n",
    "        comment_response = comment_request.execute()\n",
    "\n",
    "        for item in comment_response['items']:\n",
    "            top_comment = item['snippet']['topLevelComment']['snippet']\n",
    "            all_comments.append({\n",
    "                'Timestamp': top_comment['publishedAt'],\n",
    "                'Username': top_comment['authorDisplayName'],\n",
    "                'VideoID': video_id,\n",
    "                'Comment': top_comment['textDisplay'],\n",
    "                'Date': top_comment['updatedAt'] if 'updatedAt' in top_comment else top_comment['publishedAt']\n",
    "            })\n",
    "\n",
    "        next_page_token = comment_response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return all_comments\n",
    "\n",
    "# List to hold all comments from all videos\n",
    "all_comments = []\n",
    "\n",
    "\n",
    "for video_id in video_ids:\n",
    "    video_comments = get_comments_for_video(youtube, video_id)\n",
    "    all_comments.extend(video_comments)\n",
    "\n",
    "# Create DataFrame\n",
    "comments_df = pd.DataFrame(all_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing all the comments in the form of Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# List to hold all comments from all videos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m all_comments \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvideo_ids\u001b[49m:\n\u001b[0;32m      6\u001b[0m     video_comments \u001b[38;5;241m=\u001b[39m get_comments_for_video(youtube, video_id)\n\u001b[0;32m      7\u001b[0m     all_comments\u001b[38;5;241m.\u001b[39mextend(video_comments)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'video_ids' is not defined"
     ]
    }
   ],
   "source": [
    "# List to hold all comments from all videos\n",
    "all_comments = []\n",
    "\n",
    "\n",
    "for video_id in video_ids:\n",
    "    video_comments = get_comments_for_video(youtube, video_id)\n",
    "    all_comments.extend(video_comments)\n",
    "\n",
    "# Create DataFrame\n",
    "comments_df = pd.DataFrame(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comments_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcomments_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comments_df' is not defined"
     ]
    }
   ],
   "source": [
    "comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = 'comments_data.csv'  # Name your file\n",
    "# comments_df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comments_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomments_data2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mcomments_df\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(csv_file,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comments_df' is not defined"
     ]
    }
   ],
   "source": [
    "csv_file = 'comments_data2.csv'\n",
    "comments_df.to_csv(csv_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comments_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcomments_df\u001b[49m\u001b[38;5;241m.\u001b[39minfo()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comments_df' is not defined"
     ]
    }
   ],
   "source": [
    "comments_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('comments_data.csv')\n",
    "df2 = pd.read_csv('comments_data2.csv')\n",
    "comments_df = pd.concat([df1,df2])\n",
    "comments_df.to_csv('final_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11604 entries, 0 to 6041\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Timestamp  11604 non-null  object\n",
      " 1   Username   11604 non-null  object\n",
      " 2   VideoID    11604 non-null  object\n",
      " 3   Comment    11590 non-null  object\n",
      " 4   Date       11604 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 543.9+ KB\n"
     ]
    }
   ],
   "source": [
    "comments_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Class to each and every comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RUSHABH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RUSHABH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define lists of keywords for each class\n",
    "positive_keywords = ['great', 'amazing', 'excellent', 'good', 'appreciate', 'thank','best','awesome']\n",
    "negative_keywords = ['bad', 'terrible', 'worst', 'not able', 'could not', 'fail']\n",
    "interrogative_keywords = ['what', 'why', 'how', 'when', 'where', 'is', 'are', 'do', 'does', 'can', 'could']\n",
    "imperative_keywords = ['let', 'try', 'follow', 'do', 'make', 'watch', 'start', 'stop']\n",
    "corrective_keywords = ['correction', 'mistake', 'error', 'fix', 'remedy', 'improve']\n",
    "\n",
    "# Define stop words and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Check if the input is a string\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words and stem the remaining words\n",
    "    filtered_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "def classify_comment(comment_text):\n",
    "    # Check if the input is a string and not empty\n",
    "    if not isinstance(comment_text, str) or not comment_text.strip():\n",
    "        return 'Miscellaneous'\n",
    "\n",
    "    tokens = preprocess_text(comment_text)\n",
    "\n",
    "    if any(keyword in tokens for keyword in positive_keywords):\n",
    "        return 'Positive'\n",
    "    elif any(keyword in tokens for keyword in negative_keywords):\n",
    "        return 'Negative'\n",
    "    elif any(keyword in tokens for keyword in interrogative_keywords):\n",
    "        return 'Interrogative'\n",
    "    elif any(keyword in tokens for keyword in imperative_keywords):\n",
    "        return 'Imperative'\n",
    "    elif any(keyword in tokens for keyword in corrective_keywords):\n",
    "        return 'Corrective'\n",
    "    else:\n",
    "        return 'Miscellaneous'\n",
    "\n",
    "# Add a new column 'Class' to the DataFrame\n",
    "comments_df['Class'] = comments_df['Comment'].apply(classify_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11604 entries, 0 to 6041\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Timestamp  11604 non-null  object\n",
      " 1   Username   11604 non-null  object\n",
      " 2   VideoID    11604 non-null  object\n",
      " 3   Comment    11590 non-null  object\n",
      " 4   Date       11604 non-null  object\n",
      " 5   Class      11604 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 634.6+ KB\n"
     ]
    }
   ],
   "source": [
    "comments_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Username</th>\n",
       "      <th>VideoID</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-09T16:49:49Z</td>\n",
       "      <td>@princekhunt1</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>Best course ever</td>\n",
       "      <td>2024-04-09T16:49:49Z</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-05T18:44:47Z</td>\n",
       "      <td>@hashamqureshi6408</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>I am someone with no tech background but want ...</td>\n",
       "      <td>2024-04-05T18:44:47Z</td>\n",
       "      <td>Imperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-24T18:10:58Z</td>\n",
       "      <td>@muhammadarhamasif2566</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>Sir you make me fall in love with mathematics ...</td>\n",
       "      <td>2024-03-24T18:10:58Z</td>\n",
       "      <td>Imperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-21T06:10:36Z</td>\n",
       "      <td>@jatinnandwani6678</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>Thanks so much</td>\n",
       "      <td>2024-03-21T06:10:36Z</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-14T15:14:39Z</td>\n",
       "      <td>@shashankshekharsingh9336</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>let's go</td>\n",
       "      <td>2024-03-14T15:14:48Z</td>\n",
       "      <td>Imperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-03-13T08:07:55Z</td>\n",
       "      <td>@saifabbas1991</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>❤</td>\n",
       "      <td>2024-03-13T08:07:55Z</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-03-08T14:54:59Z</td>\n",
       "      <td>@SLADE-VA</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>ANN\\r\\na. Basics\\r\\n•\\tWhat is Deep Learning\\r...</td>\n",
       "      <td>2024-03-08T14:54:59Z</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-03-05T04:51:20Z</td>\n",
       "      <td>@svs9610</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>HI CampusX team, I have 1 question.\\n\\n Gans &amp;...</td>\n",
       "      <td>2024-03-05T04:51:44Z</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-03-02T08:52:24Z</td>\n",
       "      <td>@amankumarsahu9097</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>Hello Sir, \\ngotta give props to Campus X's 10...</td>\n",
       "      <td>2024-03-02T08:52:24Z</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-02-27T03:07:05Z</td>\n",
       "      <td>@KumR</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>Awesome</td>\n",
       "      <td>2024-02-27T03:07:05Z</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Timestamp                   Username      VideoID  \\\n",
       "0  2024-04-09T16:49:49Z              @princekhunt1  2dH_qjc9mFg   \n",
       "1  2024-04-05T18:44:47Z         @hashamqureshi6408  2dH_qjc9mFg   \n",
       "2  2024-03-24T18:10:58Z     @muhammadarhamasif2566  2dH_qjc9mFg   \n",
       "3  2024-03-21T06:10:36Z         @jatinnandwani6678  2dH_qjc9mFg   \n",
       "4  2024-03-14T15:14:39Z  @shashankshekharsingh9336  2dH_qjc9mFg   \n",
       "5  2024-03-13T08:07:55Z             @saifabbas1991  2dH_qjc9mFg   \n",
       "6  2024-03-08T14:54:59Z                  @SLADE-VA  2dH_qjc9mFg   \n",
       "7  2024-03-05T04:51:20Z                   @svs9610  2dH_qjc9mFg   \n",
       "8  2024-03-02T08:52:24Z         @amankumarsahu9097  2dH_qjc9mFg   \n",
       "9  2024-02-27T03:07:05Z                      @KumR  2dH_qjc9mFg   \n",
       "\n",
       "                                             Comment                  Date  \\\n",
       "0                                   Best course ever  2024-04-09T16:49:49Z   \n",
       "1  I am someone with no tech background but want ...  2024-04-05T18:44:47Z   \n",
       "2  Sir you make me fall in love with mathematics ...  2024-03-24T18:10:58Z   \n",
       "3                                     Thanks so much  2024-03-21T06:10:36Z   \n",
       "4                                           let's go  2024-03-14T15:14:48Z   \n",
       "5                                                  ❤  2024-03-13T08:07:55Z   \n",
       "6  ANN\\r\\na. Basics\\r\\n•\\tWhat is Deep Learning\\r...  2024-03-08T14:54:59Z   \n",
       "7  HI CampusX team, I have 1 question.\\n\\n Gans &...  2024-03-05T04:51:44Z   \n",
       "8  Hello Sir, \\ngotta give props to Campus X's 10...  2024-03-02T08:52:24Z   \n",
       "9                                            Awesome  2024-02-27T03:07:05Z   \n",
       "\n",
       "           Class  \n",
       "0       Positive  \n",
       "1     Imperative  \n",
       "2     Imperative  \n",
       "3       Positive  \n",
       "4     Imperative  \n",
       "5  Miscellaneous  \n",
       "6  Miscellaneous  \n",
       "7  Miscellaneous  \n",
       "8       Positive  \n",
       "9  Miscellaneous  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'class_comments_data.csv'\n",
    "comments_df.to_csv(csv_file,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Pre-Processing\n",
    "\n",
    "- lowercasing\n",
    "- removing URLs\n",
    "- removing new line character (\"\\n\")\n",
    "- removing punctuations\n",
    "- removing integers\n",
    "- removing emojis\n",
    "- correcting spelling errors\n",
    "- lemmatizing\n",
    "- removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RUSHABH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\RUSHABH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\RUSHABH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\RUSHABH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\RUSHABH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import unicodedata\n",
    "import contractions\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def remove_newlines(text):\n",
    "    return text.replace('\\n', ' ')\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def remove_integers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return ''.join(c for c in text if c in ['\\U0000fe0f'] or unicodedata.category(c) != 'So')\n",
    "\n",
    "def correct_spelling(text):\n",
    "    corrected_text = contractions.fix(text)\n",
    "    return corrected_text\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Check if the input is a string\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "\n",
    "    # Remove integers\n",
    "    text = remove_integers(text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = remove_urls(text)\n",
    "\n",
    "    # Remove new line characters\n",
    "    text = remove_newlines(text)\n",
    "\n",
    "    # Remove punctuations\n",
    "    text = remove_punctuations(text)\n",
    "\n",
    "    # Remove emojis\n",
    "    text = remove_emojis(text)\n",
    "\n",
    "    # Correct spelling\n",
    "    text = correct_spelling(text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [word for word in lemmatized_tokens if word not in stop_words]\n",
    "\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('class_comments_data.csv')\n",
    "\n",
    "# Preprocess the comments and add a new column to the DataFrame\n",
    "df['Comment'] = df['Comment'].apply(preprocess_text)\n",
    "\n",
    "# Save the preprocessed data to a new CSV file\n",
    "df.to_csv('preprocessed_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert text into features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Username</th>\n",
       "      <th>VideoID</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Date</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-09T16:49:49Z</td>\n",
       "      <td>@princekhunt1</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>best course ever</td>\n",
       "      <td>2024-04-09T16:49:49Z</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-05T18:44:47Z</td>\n",
       "      <td>@hashamqureshi6408</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>someone tech background want start someone rec...</td>\n",
       "      <td>2024-04-05T18:44:47Z</td>\n",
       "      <td>Imperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-03-24T18:10:58Z</td>\n",
       "      <td>@muhammadarhamasif2566</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>sir make fall love mathematics machine learn</td>\n",
       "      <td>2024-03-24T18:10:58Z</td>\n",
       "      <td>Imperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-03-21T06:10:36Z</td>\n",
       "      <td>@jatinnandwani6678</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>thanks much</td>\n",
       "      <td>2024-03-21T06:10:36Z</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-03-14T15:14:39Z</td>\n",
       "      <td>@shashankshekharsingh9336</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>let u go</td>\n",
       "      <td>2024-03-14T15:14:48Z</td>\n",
       "      <td>Imperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-03-13T08:07:55Z</td>\n",
       "      <td>@saifabbas1991</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-03-13T08:07:55Z</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-03-08T14:54:59Z</td>\n",
       "      <td>@SLADE-VA</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>ann basic deep learn deep learn v machine lear...</td>\n",
       "      <td>2024-03-08T14:54:59Z</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-03-05T04:51:20Z</td>\n",
       "      <td>@svs9610</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>hi campusx team question gans nlp include day</td>\n",
       "      <td>2024-03-05T04:51:44Z</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-03-02T08:52:24Z</td>\n",
       "      <td>@amankumarsahu9097</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>hello sir get give prop campus x day ml playli...</td>\n",
       "      <td>2024-03-02T08:52:24Z</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-02-27T03:07:05Z</td>\n",
       "      <td>@KumR</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>awesome</td>\n",
       "      <td>2024-02-27T03:07:05Z</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-02-06T06:12:50Z</td>\n",
       "      <td>@basuutube</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>another request enter generative ai well pleas...</td>\n",
       "      <td>2024-02-06T06:12:50Z</td>\n",
       "      <td>Imperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-02-06T06:09:45Z</td>\n",
       "      <td>@basuutube</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>please include dropout layer please share gith...</td>\n",
       "      <td>2024-02-06T06:09:45Z</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-02-03T09:09:17Z</td>\n",
       "      <td>@SainikDarpan</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>great work thanks</td>\n",
       "      <td>2024-02-03T09:09:17Z</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-02-02T13:19:49Z</td>\n",
       "      <td>@nishantgoyal6657</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>really appreciate deliverable</td>\n",
       "      <td>2024-02-02T13:19:49Z</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-01-22T20:11:32Z</td>\n",
       "      <td>@ammarkamal2750</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>bro great effort keep go</td>\n",
       "      <td>2024-01-22T20:11:32Z</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2024-01-16T02:28:54Z</td>\n",
       "      <td>@muhannedalogaidi7877</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>another version video english</td>\n",
       "      <td>2024-01-16T02:28:54Z</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024-01-07T05:15:54Z</td>\n",
       "      <td>@Apki_Diary_Sey</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>black board invisible see understand able plea...</td>\n",
       "      <td>2024-01-07T05:15:54Z</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024-01-05T16:51:06Z</td>\n",
       "      <td>@ASHISHMISHRA-qd5sz</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>hi sir please tell remain content deep learn a...</td>\n",
       "      <td>2024-01-05T16:51:06Z</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2023-12-09T18:13:44Z</td>\n",
       "      <td>@saliq7</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>anyone pdf note playlist</td>\n",
       "      <td>2023-12-09T18:13:44Z</td>\n",
       "      <td>Miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2023-12-03T10:38:31Z</td>\n",
       "      <td>@user-jt1tu5fu9b</td>\n",
       "      <td>2dH_qjc9mFg</td>\n",
       "      <td>sir today start course please give best wish c...</td>\n",
       "      <td>2023-12-03T10:38:31Z</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Timestamp                   Username      VideoID  \\\n",
       "0   2024-04-09T16:49:49Z              @princekhunt1  2dH_qjc9mFg   \n",
       "1   2024-04-05T18:44:47Z         @hashamqureshi6408  2dH_qjc9mFg   \n",
       "2   2024-03-24T18:10:58Z     @muhammadarhamasif2566  2dH_qjc9mFg   \n",
       "3   2024-03-21T06:10:36Z         @jatinnandwani6678  2dH_qjc9mFg   \n",
       "4   2024-03-14T15:14:39Z  @shashankshekharsingh9336  2dH_qjc9mFg   \n",
       "5   2024-03-13T08:07:55Z             @saifabbas1991  2dH_qjc9mFg   \n",
       "6   2024-03-08T14:54:59Z                  @SLADE-VA  2dH_qjc9mFg   \n",
       "7   2024-03-05T04:51:20Z                   @svs9610  2dH_qjc9mFg   \n",
       "8   2024-03-02T08:52:24Z         @amankumarsahu9097  2dH_qjc9mFg   \n",
       "9   2024-02-27T03:07:05Z                      @KumR  2dH_qjc9mFg   \n",
       "10  2024-02-06T06:12:50Z                 @basuutube  2dH_qjc9mFg   \n",
       "11  2024-02-06T06:09:45Z                 @basuutube  2dH_qjc9mFg   \n",
       "12  2024-02-03T09:09:17Z              @SainikDarpan  2dH_qjc9mFg   \n",
       "13  2024-02-02T13:19:49Z          @nishantgoyal6657  2dH_qjc9mFg   \n",
       "14  2024-01-22T20:11:32Z            @ammarkamal2750  2dH_qjc9mFg   \n",
       "15  2024-01-16T02:28:54Z      @muhannedalogaidi7877  2dH_qjc9mFg   \n",
       "16  2024-01-07T05:15:54Z            @Apki_Diary_Sey  2dH_qjc9mFg   \n",
       "17  2024-01-05T16:51:06Z        @ASHISHMISHRA-qd5sz  2dH_qjc9mFg   \n",
       "18  2023-12-09T18:13:44Z                    @saliq7  2dH_qjc9mFg   \n",
       "19  2023-12-03T10:38:31Z           @user-jt1tu5fu9b  2dH_qjc9mFg   \n",
       "\n",
       "                                              Comment                  Date  \\\n",
       "0                                    best course ever  2024-04-09T16:49:49Z   \n",
       "1   someone tech background want start someone rec...  2024-04-05T18:44:47Z   \n",
       "2        sir make fall love mathematics machine learn  2024-03-24T18:10:58Z   \n",
       "3                                         thanks much  2024-03-21T06:10:36Z   \n",
       "4                                            let u go  2024-03-14T15:14:48Z   \n",
       "5                                                 NaN  2024-03-13T08:07:55Z   \n",
       "6   ann basic deep learn deep learn v machine lear...  2024-03-08T14:54:59Z   \n",
       "7       hi campusx team question gans nlp include day  2024-03-05T04:51:44Z   \n",
       "8   hello sir get give prop campus x day ml playli...  2024-03-02T08:52:24Z   \n",
       "9                                             awesome  2024-02-27T03:07:05Z   \n",
       "10  another request enter generative ai well pleas...  2024-02-06T06:12:50Z   \n",
       "11  please include dropout layer please share gith...  2024-02-06T06:09:45Z   \n",
       "12                                  great work thanks  2024-02-03T09:09:17Z   \n",
       "13                      really appreciate deliverable  2024-02-02T13:19:49Z   \n",
       "14                           bro great effort keep go  2024-01-22T20:11:32Z   \n",
       "15                      another version video english  2024-01-16T02:28:54Z   \n",
       "16  black board invisible see understand able plea...  2024-01-07T05:15:54Z   \n",
       "17  hi sir please tell remain content deep learn a...  2024-01-05T16:51:06Z   \n",
       "18                           anyone pdf note playlist  2023-12-09T18:13:44Z   \n",
       "19  sir today start course please give best wish c...  2023-12-03T10:38:31Z   \n",
       "\n",
       "            Class  \n",
       "0        Positive  \n",
       "1      Imperative  \n",
       "2      Imperative  \n",
       "3        Positive  \n",
       "4      Imperative  \n",
       "5   Miscellaneous  \n",
       "6   Miscellaneous  \n",
       "7   Miscellaneous  \n",
       "8        Positive  \n",
       "9   Miscellaneous  \n",
       "10     Imperative  \n",
       "11  Miscellaneous  \n",
       "12       Positive  \n",
       "13  Miscellaneous  \n",
       "14       Positive  \n",
       "15  Miscellaneous  \n",
       "16  Miscellaneous  \n",
       "17  Miscellaneous  \n",
       "18  Miscellaneous  \n",
       "19       Positive  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"preprocessed_comments.csv\")\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11604, 9650)\n",
      "(11604, 9650)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('preprocessed_comments.csv')\n",
    "\n",
    "df['Comment'] = df['Comment'].fillna('')\n",
    "\n",
    "# Initialize Document Frequency Vectorizer\n",
    "doc_freq_vectorizer = CountVectorizer()\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed comments using Document Frequency Vectorizer\n",
    "doc_freq_features = doc_freq_vectorizer.fit_transform(df['Comment'])\n",
    "print(doc_freq_features.shape)\n",
    "\n",
    "# Fit and transform the preprocessed comments using TF-IDF Vectorizer\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df['Comment'])\n",
    "print(tfidf_features.shape)\n",
    "\n",
    "# Convert the feature matrices into pandas DataFrames (optional)\n",
    "doc_freq_df = pd.DataFrame(doc_freq_features.toarray(), columns=doc_freq_vectorizer.get_feature_names_out())\n",
    "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Save the features to CSV files (optional)\n",
    "doc_freq_df.to_csv('document_frequency_features.csv', index=False)\n",
    "tfidf_df.to_csv('tfidf_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Frequency Features: (11604, 9650)\n",
      "TF-IDF Features: (11604, 9650)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Read the preprocessed comments from the CSV file\n",
    "df = pd.read_csv('preprocessed_comments.csv')\n",
    "\n",
    "# Replace NaN values with empty strings\n",
    "df['Comment'] = df['Comment'].fillna('')\n",
    "\n",
    "# Convert preprocessed comments to a list\n",
    "preprocessed_comments = df['Comment'].tolist()\n",
    "\n",
    "# Document Frequency Vectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_count = count_vectorizer.fit_transform(preprocessed_comments)\n",
    "print(\"Document Frequency Features:\", X_count.shape)\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(preprocessed_comments)\n",
    "print(\"TF-IDF Features:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load preprocessed data\n",
    "df = pd.read_csv('preprocessed_comments.csv')\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Separate features (preprocessed text) and labels\n",
    "features = df['Comment']\n",
    "labels = df['Class']\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed comments using TF-IDF Vectorizer\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(features)\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    RandomForestClassifier(max_features='log2', n_estimators=1000, criterion='entropy', random_state=0),\n",
    "    LinearSVC(C=1.0, random_state=0),\n",
    "    MultinomialNB(alpha=1, fit_prior=True),\n",
    "    LogisticRegression(C=1.0, penalty='l2', solver='newton-cg', random_state=0),\n",
    "    DecisionTreeClassifier(criterion='gini', max_features=None, min_samples_leaf=1, min_samples_split=2, random_state=0),\n",
    "]\n",
    "\n",
    "# Cross-validation\n",
    "cv_df = pd.DataFrame()\n",
    "entries = []\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, tfidf_features, labels, scoring='accuracy', cv=10)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "# Plotting\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, size=10, jitter=True, linewidth=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "buffer source array is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 463, in _process_worker\n    r = call_item()\n        ^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\joblib\\parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\RUSHABH\\AppData\\Local\\Temp\\ipykernel_17144\\1857590239.py\", line 34, in preprocess_text\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\spacy\\language.py\", line 1054, in __call__\n    error_handler(name, proc, [doc], e)\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\spacy\\util.py\", line 1722, in raise_error\n    raise e\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\spacy\\language.py\", line 1049, in __call__\n    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"spacy\\pipeline\\trainable_pipe.pyx\", line 56, in spacy.pipeline.trainable_pipe.TrainablePipe.__call__\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\spacy\\util.py\", line 1722, in raise_error\n    raise e\n  File \"spacy\\pipeline\\trainable_pipe.pyx\", line 52, in spacy.pipeline.trainable_pipe.TrainablePipe.__call__\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\spacy\\pipeline\\tok2vec.py\", line 126, in predict\n    tokvecs = self.model.predict(docs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\thinc\\model.py\", line 334, in predict\n    return self._func(self, X, is_train=False)[0]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\thinc\\layers\\chain.py\", line 54, in forward\n    Y, inc_layer_grad = layer(X, is_train=is_train)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\thinc\\model.py\", line 310, in __call__\n    return self._func(self, X, is_train=is_train)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\thinc\\layers\\chain.py\", line 54, in forward\n    Y, inc_layer_grad = layer(X, is_train=is_train)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\thinc\\model.py\", line 310, in __call__\n    return self._func(self, X, is_train=is_train)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\thinc\\layers\\with_array.py\", line 36, in forward\n    return cast(Tuple[SeqT, Callable], _ragged_forward(model, Xseq, is_train))\n                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\thinc\\layers\\with_array.py\", line 91, in _ragged_forward\n    Y, get_dX = layer(Xr.dataXd, is_train)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\thinc\\model.py\", line 310, in __call__\n    return self._func(self, X, is_train=is_train)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\thinc\\layers\\concatenate.py\", line 57, in forward\n    Ys, callbacks = zip(*[layer(X, is_train=is_train) for layer in model.layers])\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\thinc\\layers\\concatenate.py\", line 57, in <listcomp>\n    Ys, callbacks = zip(*[layer(X, is_train=is_train) for layer in model.layers])\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\thinc\\model.py\", line 310, in __call__\n    return self._func(self, X, is_train=is_train)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\thinc\\layers\\chain.py\", line 54, in forward\n    Y, inc_layer_grad = layer(X, is_train=is_train)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\thinc\\model.py\", line 310, in __call__\n    return self._func(self, X, is_train=is_train)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\thinc\\layers\\hashembed.py\", line 72, in forward\n    output = model.ops.gather_add(vectors, keys)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"thinc\\backends\\numpy_ops.pyx\", line 460, in thinc.backends.numpy_ops.NumpyOps.gather_add\n  File \"stringsource\", line 660, in View.MemoryView.memoryview_cwrapper\n  File \"stringsource\", line 350, in View.MemoryView.memoryview.__cinit__\nValueError: buffer source array is read-only\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mis_stop])\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Preprocess the text in parallel using joblib\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m preprocessed_features \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_cores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Convert the preprocessed_features list to a numpy array\u001b[39;00m\n\u001b[0;32m     43\u001b[0m preprocessed_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(preprocessed_features)\n",
      "File \u001b[1;32md:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[1;32md:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\joblib\\parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1748\u001b[0m \n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[1;32md:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\joblib\\parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediatly raise the error by\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\joblib\\parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\MTECH\\2-Semester\\ML\\ML_Project\\YT_Comments_Sentiment_Analysis\\venv\\Lib\\site-packages\\joblib\\parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: buffer source array is read-only"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('preprocessed_comments.csv')\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df.dropna(subset=['Comment', 'Class'], inplace=True)\n",
    "\n",
    "# Separate features (preprocessed text) and labels\n",
    "features = df['Comment']\n",
    "labels = df['Class']\n",
    "\n",
    "# Initialize spaCy for parallel processing\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Define the number of CPU cores for parallel processing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Function to preprocess text using spaCy in parallel\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if not token.is_stop])\n",
    "\n",
    "# Preprocess the text in parallel using joblib\n",
    "preprocessed_features = Parallel(n_jobs=num_cores)(\n",
    "    delayed(preprocess_text)(text) for text in features\n",
    ")\n",
    "\n",
    "# Convert the preprocessed_features list to a numpy array\n",
    "preprocessed_features = np.array(preprocessed_features)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed comments using TF-IDF Vectorizer\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(preprocessed_features)\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    RandomForestClassifier(max_features='log2', n_estimators=1000, criterion='entropy', random_state=0),\n",
    "    LinearSVC(C=1.0, random_state=0),\n",
    "    MultinomialNB(alpha=1, fit_prior=True),\n",
    "    LogisticRegression(C=1.0, penalty='l2', solver='newton-cg', random_state=0),\n",
    "    DecisionTreeClassifier(criterion='gini', max_features=None, min_samples_leaf=1, min_samples_split=2, random_state=0),\n",
    "]\n",
    "\n",
    "# Cross-validation\n",
    "cv_df = pd.DataFrame()\n",
    "entries = []\n",
    "\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, tfidf_features, labels, scoring='accuracy', cv=10)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "# Plotting\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, size=10, jitter=True, linewidth=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
